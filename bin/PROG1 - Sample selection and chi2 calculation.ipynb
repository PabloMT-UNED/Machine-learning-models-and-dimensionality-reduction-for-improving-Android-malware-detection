{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import sys\n",
    "from threading import current_thread, Lock, Thread\n",
    "import queue\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Variables de control\n",
    "# --------------------\n",
    "\n",
    "#Utilizada para realizar pruebas\n",
    "test=1\n",
    "\n",
    "#Variable que se utiliza para diferenciar entre laboratorios. En el TFG solo hay 1, de cara al futuro el 2 sera con familias de malware.\n",
    "tipo_lab=1 #NO TOCAR, dejar en 1\n",
    "\n",
    "tipo_dataset=['manifest', 'all']\n",
    "features_manifest = ['feature','permission','provider','activity','service_receiver','intent']\n",
    "\n",
    "# -----\n",
    "# RUTAS\n",
    "# -----\n",
    "path_bin = os.getcwd()\n",
    "parent_path = Path(path_bin).parent.absolute()\n",
    "\n",
    "#Variable HOME\n",
    "if test:\n",
    "    HOME=parent_path.joinpath('TEST')\n",
    "    print (HOME)\n",
    "else:\n",
    "    HOME=parent_path\n",
    "    print (HOME)\n",
    "\n",
    "#IN=Path('F:\\TFG\\in')\n",
    "IN=HOME.joinpath('in')\n",
    "IN_CHI=IN.joinpath('logs_chi')\n",
    "\n",
    "if test:\n",
    "    dir_features = IN.joinpath('feature_vectors_test')\n",
    "else:\n",
    "    dir_features = IN.joinpath('feature_vectors')\n",
    "\n",
    "dir_malware = IN.joinpath('dataset_splits','leave-one-family-out','10-samples')\n",
    "\n",
    "#TTV para Train/test/validation\n",
    "dir_partition = IN.joinpath('TTV')\n",
    "dir_particion_20 = dir_partition.joinpath('files_20') \n",
    "dir_particion_80 = dir_partition.joinpath('files_80')\n",
    "\n",
    "\n",
    "#Ruta dicheros chi2. Falta concatenar el experimento 1,2 o 3, ya dentro del bucle\n",
    "dir_logs_chi2 = IN_CHI.joinpath('lab'+ str(tipo_lab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (path_bin)\n",
    "print (parent_path)\n",
    "print (HOME)\n",
    "print (dir_features)\n",
    "print (dir_malware)\n",
    "print (dir_logs_chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobaciones_iniciales():\n",
    "    if not Path(dir_features).exists():\n",
    "        print(\"No existe el directorio con las muestras del dataset, no se puede continuar\")\n",
    "        print(\"Directorio: \" + str(dir_features))\n",
    "        sys.exit()\n",
    "\n",
    "    if not os.listdir(dir_features):\n",
    "        print(\"Directorio de las muestras del dataset está vacío, no se puede continuar\")\n",
    "        print(\"Directorio: \" + str(dir_features))\n",
    "        sys.exit()\n",
    "\n",
    "    if not Path(dir_malware).exists():\n",
    "        print(\"No existe el directorio con la información de malware de las muestras, no se puede continuar\")\n",
    "        print(\"Directorio: \" + str(dir_malware))\n",
    "        sys.exit()\n",
    "\n",
    "    if not os.listdir(dir_malware):\n",
    "        print(\"Directorio con la información de malware de las muestras está vacío, no se puede continuar\")\n",
    "        print(\"Directorio: \" + str(dir_malware))\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprueba_directorios(*args):\n",
    "    for directorio in args:\n",
    "        #print(directorio)\n",
    "        os.makedirs(directorio,exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nombres_muestras(dir_features):\n",
    "    lista_ret=[]\n",
    "    orden=0\n",
    "    for nombre in os.listdir(dir_features):\n",
    "        if os.path.isfile(os.path.join(dir_features, nombre)):\n",
    "            lista_ret.append([orden,nombre])\n",
    "            orden+=1\n",
    "            \n",
    "    return lista_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_muestras_malware(dir_malware):\n",
    "    \n",
    "    df_muestras = pd.DataFrame()\n",
    "    \n",
    "    #Recorre cada carpeta de las pruebas de las 20 familias para extraer todos los regitros\n",
    "    for name in os.listdir(dir_malware):\n",
    "        if os.path.isdir(os.path.join(dir_malware, name)):\n",
    "            dir_nuevo = os.path.join(dir_malware, name)\n",
    "            for nombre_fichero in os.listdir(dir_nuevo):\n",
    "                #if name_f == \"test_cs\" or name_f == \"validate_cs\":\n",
    "                if nombre_fichero == \"validate_cs\":\n",
    "                    temp_df = pd.read_csv(os.path.join(dir_nuevo, nombre_fichero), header=None)\n",
    "                    df_muestras = pd.concat([df_muestras, temp_df])\n",
    "                \n",
    "    df_muestras = df_muestras.drop_duplicates()\n",
    "    df_muestras = df_muestras.fillna('NO_MW')\n",
    "    df_muestras.columns=['APP_HASH', 'FAMILY_MALWARE']\n",
    "    \n",
    "    return df_muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_etiquetado(lista_app):\n",
    "\n",
    "    df_app = pd.DataFrame(lista_app, columns=['ORDEN_APP','APP_HASH'])\n",
    "    df_app = df_app.sort_values(['ORDEN_APP'])\n",
    "    \n",
    "    df_muestras = get_muestras_malware(dir_malware)\n",
    "    \n",
    "    df_app = pd.merge(left=df_app,right=df_muestras, how='left', left_on='APP_HASH', right_on='APP_HASH')\n",
    "    \n",
    "    df_app['COD_MALWARE'] = df_app['FAMILY_MALWARE'].to_list()\n",
    "   \n",
    "    df_app['COD_MALWARE']=df_app['COD_MALWARE'].replace(to_replace=\"NO_MW\", value=np.uint8(0))\n",
    "    df_app['COD_MALWARE']=df_app['COD_MALWARE'].replace(to_replace=\"[A-Za-z]*\", value=np.uint8(1), regex=True).astype(np.uint8)\n",
    "        \n",
    "    return df_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_particiones(df_etiquetas, new_20=False, new_80=False):\n",
    "    \n",
    "    if new_20 == False and new_80 == True:\n",
    "\n",
    "        df_muestras_20 = pd.read_csv(dir_particion_20)\n",
    "        lista_20 = df_muestras_20['APP_HASH'].to_list()\n",
    "        \n",
    "        df_muestras_80 = df_etiquetas[~df_etiquetas.APP_HASH.isin(lista_20)]\n",
    "        \n",
    "        comprueba_directorios(Path(dir_particion_80).parent.absolute())\n",
    "        df_muestras_80.to_csv(dir_particion_80, index=False)\n",
    "    \n",
    "    if new_20 == True and new_80 == False:\n",
    "        \n",
    "        df_muestras_80 = pd.read_csv(dir_particion_80)\n",
    "        lista_80 = df_muestras_80['APP_HASH'].to_list()\n",
    "        \n",
    "        df_muestras_20 = df_etiquetas[~df_etiquetas.APP_HASH.isin(lista_80)]\n",
    "        comprueba_directorios(Path(dir_particion_20).parent.absolute())\n",
    "        df_muestras_20.to_csv(dir_particion_20, index=False)\n",
    "    \n",
    "    if new_20 == True and new_80 == True:\n",
    "        \n",
    "        #Partimos el dataset para los experimentos 2 y 3\n",
    "        df_muestras_20 = df_etiquetas.sample(frac=0.2, random_state=2)\n",
    "        comprueba_directorios(Path(dir_particion_20).parent.absolute())\n",
    "        df_muestras_20.to_csv(dir_particion_20, index=False)\n",
    "\n",
    "        lista_20 = df_muestras_20['APP_HASH'].to_list()\n",
    "\n",
    "        df_muestras_80 = df_etiquetas[~df_etiquetas.APP_HASH.isin(lista_20)]\n",
    "        comprueba_directorios(Path(dir_particion_80).parent.absolute())\n",
    "        df_muestras_80.to_csv(dir_particion_80, index=False)\n",
    "    \n",
    "    return df_muestras_80, df_muestras_20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nombre_fichero_chi2(dir_logs_chi2, usar_features, tipo_lab, experimento, iteracion, test):\n",
    "    \n",
    "    nombre_base_chi = 'chi2_' + usar_features + '_L' + str(tipo_lab) + '_E' + str(experimento) + '_I' + str(iteracion) +'_T' + str(test)\n",
    "    \n",
    "    nombre_fichero_chi = dir_logs_chi2.joinpath(nombre_base_chi)\n",
    "    nombre_fichero_chi_apks_train = dir_logs_chi2.joinpath(nombre_base_chi + \"_list_apks_TRAIN\")\n",
    "    nombre_fichero_chi_apks_test = dir_logs_chi2.joinpath(nombre_base_chi + \"_list_apks_TEST\")\n",
    "    \n",
    "    return nombre_fichero_chi, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_ficheros_listados_apk(experimento, mw_etiq_val_train, mw_etiq_val_test, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test):\n",
    "    \n",
    "    if experimento != 3:\n",
    "        #Hashes de las apps involucradas en el cálculo\n",
    "        \n",
    "        #Muestras train - Reseteamos el orden a 0\n",
    "        df_temp_train = pd.DataFrame(mw_etiq_val_train, columns=['ORDEN_APP','APP_HASH','FAMILY_MALWARE','Y_TEST'])\n",
    " \n",
    "        df_temp_train = df_temp_train[['APP_HASH','FAMILY_MALWARE','Y_TEST']].reset_index()\n",
    "        \n",
    "        comprueba_directorios(Path(nombre_fichero_chi_apks_train).parent.absolute())\n",
    "        df_temp_train[['index','APP_HASH','FAMILY_MALWARE','Y_TEST']].to_csv(nombre_fichero_chi_apks_train, index=False)\n",
    "\n",
    "        #Muestras test - Reseteamos el orden a 0\n",
    "        df_temp_test = pd.DataFrame(mw_etiq_val_test, columns=['ORDEN_APP','APP_HASH','FAMILY_MALWARE','Y_TEST'])\n",
    "\n",
    "        df_temp_test = df_temp_test[['APP_HASH','FAMILY_MALWARE','Y_TEST']].reset_index()\n",
    "        \n",
    "        comprueba_directorios(Path(nombre_fichero_chi_apks_test).parent.absolute())\n",
    "        df_temp_test[['index','APP_HASH','FAMILY_MALWARE','Y_TEST']].to_csv(nombre_fichero_chi_apks_test, index=False)\n",
    "        \n",
    "        del df_temp_train, df_temp_test \n",
    "        \n",
    "    else:\n",
    "        #El E3 solo tiene test - Reseteamos el orden a 0\n",
    "        df_temp_test = pd.DataFrame(mw_etiq_val_test, columns=['ORDEN_APP','APP_HASH','FAMILY_MALWARE','Y_TEST'])\n",
    "        \n",
    "        df_temp_test = df_temp_test[['APP_HASH','FAMILY_MALWARE','Y_TEST']].reset_index()\n",
    "        comprueba_directorios(Path(nombre_fichero_chi_apks_test).parent.absolute())\n",
    "        df_temp_test[['index','APP_HASH','FAMILY_MALWARE','Y_TEST']].to_csv(nombre_fichero_chi_apks_test, index=False)\n",
    "    \n",
    "        del df_temp_test\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar_ficheros_particiones(df_etiquetas):\n",
    "    \n",
    "    #Primero comprobamos si existe el fichero\n",
    "    aux_20 = dir_particion_20.exists()\n",
    "    aux_80 = dir_particion_80.exists()\n",
    "    \n",
    "    #En caso de no existir alguno, llamamos a crear ficheros con la orden de crear o no\n",
    "    if not aux_20 or not aux_80:\n",
    "        df_muestras_80, df_muestras_20 = crear_particiones(df_etiquetas, new_20=not aux_20, new_80=not aux_80)  \n",
    "    else:\n",
    "        df_muestras_80 = pd.read_csv(dir_particion_80)\n",
    "        df_muestras_20 = pd.read_csv(dir_particion_20)\n",
    "        \n",
    "    return df_muestras_80, df_muestras_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_ficheros_iteraciones(df_etiquetas,df_muestras_80, df_muestras_20):\n",
    "\n",
    "    for usar_features in tipo_dataset:\n",
    "        #Se consideran 3 experimentos. En el tercero solo se guardan nombres para test\n",
    "        for experimento in range(1,4):\n",
    "\n",
    "            kf = KFold(n_splits=10) #10% para test\n",
    "            iteracion = 1\n",
    "\n",
    "            dir_logs_chi2_temp = dir_logs_chi2.joinpath('exp' + str(experimento))\n",
    "\n",
    "            if experimento == 1:\n",
    "\n",
    "                for i_train, i_test in kf.split(df_etiquetas):\n",
    "\n",
    "                    nombre_fichero_chi, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test = get_nombre_fichero_chi2(dir_logs_chi2_temp, usar_features, tipo_lab, experimento, iteracion, test)\n",
    "\n",
    "                    mw_etiq_val_train = df_etiquetas.values[i_train]\n",
    "                    mw_etiq_val_test = df_etiquetas.values[i_test]\n",
    "\n",
    "                    crear_ficheros_listados_apk(experimento, mw_etiq_val_train, mw_etiq_val_test, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test)\n",
    "\n",
    "                    iteracion += 1\n",
    "\n",
    "            if experimento == 2:\n",
    "\n",
    "                for i_train, i_test in kf.split(df_muestras_80):\n",
    "\n",
    "                    nombre_fichero_chi, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test = get_nombre_fichero_chi2(dir_logs_chi2_temp, usar_features, tipo_lab, experimento, iteracion, test)\n",
    "\n",
    "                    mw_etiq_val_train = df_muestras_80.values[i_train]\n",
    "                    mw_etiq_val_test = df_muestras_80.values[i_test]\n",
    "\n",
    "                    crear_ficheros_listados_apk(experimento, mw_etiq_val_train, mw_etiq_val_test, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test)\n",
    "\n",
    "                    iteracion += 1\n",
    "\n",
    "            if experimento == 3:\n",
    "                nombre_fichero_chi, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test = get_nombre_fichero_chi2(dir_logs_chi2_temp, usar_features, tipo_lab, experimento, iteracion, test)\n",
    "\n",
    "                mw_etiq_val_train = []\n",
    "                mw_etiq_val_test = df_muestras_20.values\n",
    "\n",
    "                crear_ficheros_listados_apk(experimento, mw_etiq_val_train, mw_etiq_val_test, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar_ficheros_iteraciones(df_etiquetas,df_muestras_80, df_muestras_20):\n",
    "    seguir = True\n",
    "    #40 Ficheros E1 manifest y all, 40 para E2 manifest y all y 2 para E3\n",
    "    total_ficheros=82\n",
    "    cont_fallos=0\n",
    "\n",
    "    for usar_features in tipo_dataset:\n",
    "        #Se consideran 3 experimentos. En el tercero solo se guardan nombres para test\n",
    "        for experimento in range(1,4):\n",
    "\n",
    "            for iteracion in range(1,11):\n",
    "\n",
    "                dir_logs_chi2_temp = dir_logs_chi2.joinpath('exp' + str(experimento))\n",
    "\n",
    "                nombre_fichero_chi, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test = get_nombre_fichero_chi2(dir_logs_chi2_temp, usar_features, tipo_lab, experimento, iteracion, test)\n",
    "\n",
    "                if experimento != 3:\n",
    "                    if not Path(nombre_fichero_chi_apks_train).exists():\n",
    "                        seguir = False\n",
    "                        cont_fallos+=1\n",
    "\n",
    "                if not Path(nombre_fichero_chi_apks_test).exists():\n",
    "                    seguir = False\n",
    "                    cont_fallos+=1\n",
    "\n",
    "                if experimento == 3:\n",
    "                    break\n",
    "     \n",
    "    #Si faltan todos los ficheros, los creamos y continuamos\n",
    "    if not seguir and cont_fallos == total_ficheros:\n",
    "        crear_ficheros_iteraciones(df_etiquetas,df_muestras_80, df_muestras_20)\n",
    "        seguir = True\n",
    "        \n",
    "    return seguir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para prevenir que varios threads escriban al mismo tiempo por pantalla.\n",
    "def impresion_segura(lock, *args):\n",
    "\n",
    "    lock.acquire()\n",
    "    print(*args)\n",
    "    lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_features(cola, lock, list_dict_features,lista_features,usar_features):#, lista_features, lista_sum_features_app, calcular_descuadre, usar_features, features_manifest):\n",
    "    \n",
    "    while True:\n",
    "        app = cola.get()\n",
    "       \n",
    "        if app is not None:\n",
    "\n",
    "            file_app = dir_features.joinpath(app[1])\n",
    "            \n",
    "            lista_features_app=[] #Guarda todas las features de la app en cada bucle\n",
    "\n",
    "\n",
    "            try:\n",
    "                with open(file_app, 'r') as f:\n",
    "                    \n",
    "                    if usar_features.strip() == 'manifest':\n",
    "                        for linea in f:\n",
    "                            #Controlamos lineas en blanco\n",
    "                            feature = linea.replace('\\n','').strip()\n",
    "                            \n",
    "                            division = feature.split(\"::\")\n",
    "                            \n",
    "                            if division[0] in features_manifest:\n",
    "\n",
    "                                lock.acquire()\n",
    "                                lista_features.append(feature) \n",
    "                                lock.release()\n",
    "                                \n",
    "                                lista_features_app.append(feature)\n",
    "\n",
    "                        \n",
    "                    else: #Obtener TODAS las features\n",
    "\n",
    "                        for linea in f:\n",
    "                            #Controlamos lineas en blanco\n",
    "                            feature = linea.replace('\\n','').strip()\n",
    "                            if feature.strip() != '':\n",
    "\n",
    "                                lock.acquire()\n",
    "                                lista_features.append(feature) \n",
    "                                lock.release()\n",
    "                                \n",
    "                                lista_features_app.append(feature)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print (str(e) + \" --- \" + file_app)\n",
    "            \n",
    "            lock.acquire()\n",
    "            list_dict_features.append({'orden': app[0], 'app': app[1], 'features': lista_features_app})\n",
    "            lock.release()\n",
    "\n",
    "        if app is None:\n",
    "            break\n",
    "    impresion_segura(lock, current_thread().name, \"terminó ejecución.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_extraer_features(cola, list_dict_features,lista_features, usar_features):\n",
    "    \n",
    "    # Para evitar la superposición en la impresión de textos.\n",
    "    lock = Lock()\n",
    "    \n",
    "    cores = cpu_count()\n",
    "    threads = []\n",
    "    \n",
    "    for i in range(cores):\n",
    "\n",
    "        thread = Thread(target=extraer_features, args=(cola, lock, list_dict_features,lista_features,usar_features))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Terminar hilos.\n",
    "    for i in range(cores):\n",
    "        cola.put(None)\n",
    "\n",
    "    # Esperar a que finalice el procesamiento de cada hilo.\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rellenar_dataset(cola_features, lock, df_dataset):\n",
    "    \n",
    "    while True:\n",
    "        dict_temp = cola_features.get()\n",
    "        \n",
    "        if dict_temp is not None:\n",
    "            \n",
    "            #Este control es debido al rellenado del dataset de test. Si no existe la feature en el train, no se considera\n",
    "            for feature in dict_temp['features']:\n",
    "                if feature in df_dataset.columns:\n",
    "\n",
    "                    lock.acquire() \n",
    "                    df_dataset.at[dict_temp['orden'], feature] = np.uint8(1)\n",
    "                    lock.release()\n",
    "\n",
    "        if dict_temp is None:\n",
    "            break\n",
    "    impresion_segura(lock, current_thread().name, \"exited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_rellenar_dataset(cola, df_dataset):\n",
    "    \n",
    "    # Para evitar la superposición en la impresión de textos.\n",
    "    lock = Lock()\n",
    "    \n",
    "    cores = cpu_count()\n",
    "    threads = []\n",
    "    \n",
    "    for i in range(cores):\n",
    "        \n",
    "        thread = Thread(target=rellenar_dataset, args=(cola, lock, df_dataset))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Terminar hilos.\n",
    "    for i in range(cores):\n",
    "        cola.put(None)\n",
    "\n",
    "    # Esperar a que finalice el procesamiento de cada hilo.\n",
    "    for t in threads:\n",
    "        t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dataset(lista_app_train, usar_features):\n",
    "    \n",
    "    list_dict_features=[]\n",
    "    lista_features=[]\n",
    "    cola_app = queue.Queue()\n",
    "    cola_app.queue = queue.deque(lista_app_train)\n",
    "\n",
    "    #Procedimiento ejecutado en procesos paralelos para hacerlo a mayor velocidad\n",
    "    procesar_extraer_features(cola_app, list_dict_features, lista_features, usar_features)\n",
    "    \n",
    "    print (\"--LEN DICT TRAIN ANTES EXTRAER: \" + str(len(list_dict_features)))\n",
    "\n",
    "    df_app_fea = pd.DataFrame(lista_features, columns=['Feature'])\n",
    "    lista_features = df_app_fea['Feature'].drop_duplicates().to_list()\n",
    "    \n",
    "    print (\"-- LEN LIST FEA: \" + str(len(lista_features)))\n",
    "    \n",
    "    #Creamos un array de \"num_apps\" como filas y \"num_features\" como columnas. Rellenamos con 0 todas las posiciones\n",
    "    array_ceros = np.zeros((len(lista_app_train), len(lista_features)), dtype=np.uint8)\n",
    "    \n",
    "    #Creamos el dataset que usaremos para calcular las chi2\n",
    "    df_dataset = pd.DataFrame(array_ceros, columns=lista_features)\n",
    "    \n",
    "    cola_features = queue.Queue()\n",
    "    cola_features.queue = queue.deque(list_dict_features)\n",
    "\n",
    "    #Procedimiento ejecutado en procesos paralelos para hacerlo a mayor velocidad\n",
    "    procesar_rellenar_dataset(cola_features, df_dataset)\n",
    "    \n",
    "    \n",
    "    del df_app_fea\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_chi2(x,y,iteracion, experimento, usar_features,nombre_fichero_chi,lista_features):\n",
    "\n",
    "    print (\"INICIO CHI2 \" + str(nombre_fichero_chi))\n",
    "    \n",
    "    feature_s = SelectKBest(score_func=chi2, k='all')\n",
    "    feature_s.fit(x, y)\n",
    "    \n",
    "    print (\"FINAL CHI2\")\n",
    "\n",
    "\n",
    "    ruta_export = dir_logs_chi2.joinpath(nombre_fichero_chi)\n",
    "    \n",
    "    with open(ruta_export, 'w') as f:\n",
    "        f.write('FEATURE¬SCORE\\n')\n",
    "        \n",
    "        for feature, score in zip(lista_features, feature_s.scores_):\n",
    "            f.write(feature + '¬' + str(score) + '\\n')\n",
    "\n",
    "    print (\"FINALIZA VOLCADO A FICHERO CHI2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INICIO DE EJECUCIÓN\n",
    "comprobaciones_iniciales()\n",
    "\n",
    "lista_app = get_nombres_muestras(dir_features)\n",
    "\n",
    "#Creamos un dataframe con la lista de aplicaciones adquirida y su clase\n",
    "df_etiquetas = crear_etiquetado(lista_app)\n",
    "\n",
    "#Comprobar si existen las particiones de 20 y 80. \n",
    "df_muestras_80, df_muestras_20 = comprobar_ficheros_particiones(df_etiquetas)\n",
    " \n",
    "if not comprobar_ficheros_iteraciones(df_etiquetas,df_muestras_80, df_muestras_20):\n",
    "    print (\"Faltan algunos ficheros de listados de aplicaciones por iteraciones. No se puede determinar si ha habido modificaciones en el dataset, no se puede continuar.\")\n",
    "    sys.exit()\n",
    "\n",
    "#EMPEZAR EJECUCIONES\n",
    "\n",
    "for usar_features in tipo_dataset:\n",
    "    #Se consideran 2 experimentos. En el tercero se ejecuta en otro programa\n",
    "    for experimento in range(1,3):\n",
    "        dir_logs_chi2_temp = dir_logs_chi2.joinpath('exp' + str(experimento))\n",
    "        \n",
    "        for iteracion in range(1,11): \n",
    "            \n",
    "            nombre_fichero_chi, nombre_fichero_chi_apks_train, nombre_fichero_chi_apks_test = get_nombre_fichero_chi2(dir_logs_chi2_temp, usar_features, tipo_lab, experimento, iteracion, test)\n",
    "            \n",
    "            if not Path(nombre_fichero_chi).exists():\n",
    "\n",
    "                #Cargamos el listado de apps para el experimento e iteracion correspondiente y tipo de dataset\n",
    "                df_apps_train = pd.read_csv(nombre_fichero_chi_apks_train)\n",
    "                \n",
    "                #Aprovechamos el index para hacer una columna para mantener el orden\n",
    "                df_apps_train = df_apps_train.reset_index()\n",
    "                \n",
    "                #Creamos un listado con los pares orden y hash para poder rellenar correctamente el dataset\n",
    "                lista_app_train = df_apps_train[['index','APP_HASH']].to_numpy().tolist()\n",
    "                \n",
    "                #Proceso de crear y rellenar el dataset\n",
    "                df_dataset = crear_dataset(lista_app_train, usar_features)\n",
    "                \n",
    "                lista_features = df_dataset.columns.to_list()\n",
    "                x = df_dataset.values\n",
    "                y = df_apps_train['Y_TEST'].tolist()\n",
    "                \n",
    "                #Extraemos valores del dataset (variables independientes) y la clase, si es malware o no (variable dependiente)\n",
    "                calcular_chi2(x,y,iteracion, experimento, usar_features,nombre_fichero_chi,lista_features)\n",
    "\n",
    "                #Borramos de memoria el dataset y limpiamos\n",
    "                del df_dataset\n",
    "    \n",
    "                gc.collect()\n",
    "            else:\n",
    "                print (\"YA EJECUTADA: \" + str(nombre_fichero_chi))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
