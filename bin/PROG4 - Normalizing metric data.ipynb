{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import operator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para calcular el tiempo medio\n",
    "def avg_datetime(series):\n",
    "    dt_min = series.min()\n",
    "    deltas = [x-dt_min for x in series]\n",
    "    return dt_min + functools.reduce(operator.add, deltas) / len(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=1\n",
    "\n",
    "# -----\n",
    "# RUTAS\n",
    "# -----\n",
    "path_bin = os.getcwd()\n",
    "parent_path = Path(path_bin).parent.absolute()\n",
    "\n",
    "#Variable HOME\n",
    "if test:\n",
    "    HOME=parent_path.joinpath('TEST')\n",
    "    print (HOME)\n",
    "else:\n",
    "    HOME=parent_path\n",
    "    print (HOME)\n",
    "\n",
    "dir_metrics = HOME.joinpath('out','metrics')\n",
    "    \n",
    "\n",
    "algoritmos = ['RL','RFO','NBA','ARD','SVM']\n",
    "tipo_dataset = [\"all\", \"manifest\"]\n",
    "tipo_exp = ['E1','E2','E3']\n",
    "\n",
    "##num_features = [50,500,5000,15000]\n",
    "num_features = [20,30,40,50]\n",
    "\n",
    "\n",
    "dir_excel_metricas = HOME.joinpath('out','metricas_finales.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos esqueleto del JSON\n",
    "\n",
    "iteraciones = [x for x in range(1,11)]\n",
    "\n",
    "dic_final = {}\n",
    "\n",
    "dic_dataset ={}\n",
    "for t_data in tipo_dataset:    \n",
    "    dic_exp = {}\n",
    "    for exp in tipo_exp:\n",
    "        dic_algoritmos = {}\n",
    "        for algoritmo in algoritmos:\n",
    "            dic_n_features = {}\n",
    "            for features in num_features:\n",
    "                dic_iteraciones = {}\n",
    "                for iteracion in iteraciones:\n",
    "                    dic_iteraciones.update({iteracion : 0})\n",
    "\n",
    "                dic_n_features.update({features : dic_iteraciones})\n",
    "\n",
    "            dic_algoritmos.update({algoritmo : dic_n_features})\n",
    "\n",
    "        dic_exp.update({exp : dic_algoritmos})\n",
    "    dic_dataset.update({t_data : dic_exp})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_l=1\n",
    "\n",
    "#Convertimos los datos de las metricas obtenidos en las iteraciones en un json comun.\n",
    "for dataset in tipo_dataset:\n",
    "\n",
    "    for exp_e in range(1,4):\n",
    "\n",
    "        ruta_metricas = dir_metrics.joinpath(\"lab\" + str(lab_l), \"exp\" + str(exp_e), \"metrics_\" + dataset + \"_test_\"+ str(test) + \".json\")\n",
    "            \n",
    "        try:            \n",
    "            \n",
    "            with open (str(ruta_metricas), 'r') as f_metrics:\n",
    "                metricas = json.load(f_metrics)\n",
    "\n",
    "            for metrica in metricas:\n",
    "\n",
    "                algoritmo = metrica['config']['algoritmo']\n",
    "                iteracion = metrica['config']['iteracion']\n",
    "                lab = 'L' + str(metrica['config']['lab'])\n",
    "                exp = 'E' + str(metrica['config']['experimento'])\n",
    "                n_fea = metrica['config']['k']\n",
    "                log = metrica['config']['log']\n",
    "                tipo = metrica['config']['dataset']\n",
    "\n",
    "                formas = metrica['formas']\n",
    "                class_report = metrica['class_report']\n",
    "                metricas_ad = metrica['metricas_ad']\n",
    "                tiempo = metrica['tiempo']['tiempo_ejecucion']\n",
    "\n",
    "                tfp = metrica['tasa_fp']\n",
    "                matriz_confusion = metrica['matriz_confusion']\n",
    "\n",
    "                #Rellenamos los valores de las iteraciones, inicializadas a 0 en un principio\n",
    "                dic_dataset[tipo][exp][algoritmo][n_fea][iteracion] = {'formas' : formas, 'matriz_confusion' : matriz_confusion, 'class_report' : class_report, 'meticas_ad' : metricas_ad, 'tiempo_ejecucion' : tiempo, 'log' : log, 'tasa_fp' : tfp}\n",
    "\n",
    "        except:\n",
    "            print(\"No existen datos de E3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabeceras=['tipo_data','exp','algoritmo','features','iteracion','shape x_train','shape x_test','shape y_train','shape y_test','MC_TN','MC_FP','MC_FN','MC_TP']\n",
    "\n",
    "#Cogemos un registro de ejemplo para rellenar las cabeceras\n",
    "muestra_cab=dic_dataset['manifest']['E1']['RL'][20][1]\n",
    "\n",
    "#Iteramos el resto de valores para rellenar las cabeceras\n",
    "for elemento in muestra_cab:\n",
    "    if elemento not in ['formas', 'matriz_confusion']:\n",
    "        if isinstance(muestra_cab[elemento], dict):\n",
    "            for key in muestra_cab[elemento]:\n",
    "                if isinstance(muestra_cab[elemento][key], dict):\n",
    "                    for key2 in muestra_cab[elemento][key]:\n",
    "                        nombre = key + ' ' + key2\n",
    "                        cabeceras.append(nombre)\n",
    "                else:\n",
    "                    cabeceras.append(key)\n",
    "        else:\n",
    "            cabeceras.append(elemento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_fichero=[]\n",
    "\n",
    "#Iteramos sobre el json para convertirlo en un dataframe \n",
    "for dataset in tipo_dataset:\n",
    "    for exp in tipo_exp:\n",
    "        for algoritmo in algoritmos:\n",
    "            for features in num_features:\n",
    "\n",
    "                lista_iter=[]\n",
    "\n",
    "                for iteracion in iteraciones:\n",
    "                    lista_iter=[dataset,exp,algoritmo,features,iteracion]\n",
    "\n",
    "                    valor_aux = dic_dataset[dataset][exp][algoritmo][features][iteracion]\n",
    "\n",
    "                    if valor_aux != 0:\n",
    "                        for data_iter in valor_aux:\n",
    "\n",
    "                            if isinstance(valor_aux[str(data_iter)], dict):\n",
    "                                for key in valor_aux[data_iter]:\n",
    "                                    if isinstance(valor_aux[data_iter][key], dict):\n",
    "                                        for key2 in valor_aux[data_iter][key]:\n",
    "                                            lista_iter.append(valor_aux[data_iter][key][key2])\n",
    "                                    else:\n",
    "                                        lista_iter.append(valor_aux[data_iter][key])\n",
    "                            else:\n",
    "                                lista_iter.append(valor_aux[data_iter])\n",
    "\n",
    "                        lista_fichero.append(lista_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = pd.DataFrame(lista_fichero, columns=cabeceras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenado = df_log.sort_values(by=['algoritmo','features','iteracion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb_ord = ordenado.groupby(['tipo_data','exp','algoritmo','features'])\n",
    "\n",
    "arr_medias = []\n",
    "\n",
    "#Creamos el dataframe con las medias de datos por experimento, tipo de dataset y algoritmo\n",
    "for t1, t2 in df_gb_ord:\n",
    "    aux_arr=[]\n",
    "    \n",
    "    aux_arr.append(t1[0])\n",
    "    aux_arr.append(t1[1])\n",
    "    aux_arr.append(t1[2])\n",
    "    aux_arr.append(t1[3])\n",
    "    \n",
    "    medidas = ['0 precision','0 recall','0 f1-score','1 precision','1 recall','1 f1-score','accuracy','macro avg precision','macro avg recall','macro avg f1-score','rocc','kappa','tasa_fp','tiempo_ejecucion']\n",
    "    \n",
    "    for medida in medidas:\n",
    "        if medida == 'tiempo_ejecucion':\n",
    "            \n",
    "            #Control para muestras muy peque√±as, cuyo tiempo es 0:00:00 sin milisegundos\n",
    "            try:\n",
    "                t2[medida] = pd.to_datetime(t2[medida], format='%H:%M:%S.%f')\n",
    "                aux_arr.append(avg_datetime(t2[medida]).strftime(\"%H:%M:%S.2%f\"))\n",
    "            except:\n",
    "                arr_tiempo=[]\n",
    "                for tiempo in t2[medida].to_list():\n",
    "                    try:\n",
    "                        arr_tiempo.append(datetime.strptime(tiempo, '%H:%M:%S.%f'))\n",
    "                    except:\n",
    "                        arr_tiempo.append(datetime.strptime(tiempo + '.000000', '%H:%M:%S.%f'))\n",
    "                \n",
    "                t2[medida] = pd.to_datetime(arr_tiempo, format='%H:%M:%S.%f')\n",
    "                aux_arr.append(avg_datetime(t2[medida]).strftime('%H:%M:%S.%f'))\n",
    "\n",
    "        else:\n",
    "            aux_arr.append(t2[medida].mean())\n",
    "    \n",
    "    arr_medias.append(aux_arr)\n",
    "\n",
    "cabeceras_med = ['tipo_data','exp','algoritmo','features','0 precision','0 recall','0 f1-score','1 precision','1 recall','1 f1-score','accuracy','macro avg precision','macro avg recall','macro avg f1-score','rocc','kappa','tasa_fp','avg_tiempo_ejecucion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos el dataframe de las medias\n",
    "df_medias = pd.DataFrame(arr_medias, columns=cabeceras_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir listados en excel\n",
    "writer = pd.ExcelWriter(dir_excel_metricas, engine='xlsxwriter')\n",
    "\n",
    "ordenado_medias = df_medias.sort_values(by=['tipo_data','exp','features','algoritmo'])\n",
    "\n",
    "ordenado.to_excel(writer, sheet_name=\"DATOS\", engine=\"xlsxwriter\", index=False)\n",
    "ordenado_medias.to_excel(writer, sheet_name=\"MEDIAS\", engine=\"xlsxwriter\", index=False)\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-protection",
   "metadata": {},
   "source": [
    "### MEJORES E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos solo los registros de E2\n",
    "df_E2 = ordenado[(ordenado.exp == 'E2')]\n",
    "\n",
    "arr_best = []\n",
    "\n",
    "for tipo in tipo_dataset:\n",
    "    for alg in algoritmos:\n",
    "        linea = df_E2[((df_E2.tipo_data == tipo) & (df_E2.algoritmo == alg))].sort_values(['macro avg f1-score'], ascending=False).iloc[0].to_list()\n",
    "        \n",
    "        arr_best.append(linea)\n",
    "        \n",
    "df_best_E2 = pd.DataFrame(arr_best, columns=cabeceras)\n",
    "\n",
    "df_best_E2.sort_values(['macro avg f1-score'], ascending=False)\n",
    "\n",
    "df_gb_best_E2 = df_best_E2.groupby(['exp','tipo_data','features','iteracion'])\n",
    "\n",
    "arr_tuplas=[]\n",
    "\n",
    "for x1, x2 in df_gb_best_E2:\n",
    "   \n",
    "    arr_tuplas.append((x1[0], x1[1], x1[2], x1[3], x2['algoritmo'].to_list()))\n",
    "    \n",
    "#Linea a exportar para ejecutar validacion en E3\n",
    "print (arr_tuplas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
